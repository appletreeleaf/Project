from loguru import logger
import streamlit as st
from utills import (print_message, get_session_history, StreamHandler,
                    get_filtered_relevant_docs)

from langchain import hub
from langchain_core.messages import ChatMessage
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, CSVLoader, TextLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.retrievers import BM25Retriever, EnsembleRetriever
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain.tools.retriever import create_retriever_tool
from langchain.agents import create_openai_functions_agent, AgentExecutor
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_teddynote.messages import stream_response

st.set_page_config(page_title="MyAssistant", page_icon="ğŸ¤—")
st.title("ğŸ¤— MyAssistant")

if "conversation_chain" not in st.session_state:
    st.session_state.conversation = None

# side bar
with st.sidebar:
    session_id = st.text_input("Session ID", value="Chating Room")
    uploaded_file = st.file_uploader("Upload a file", type=['pdf', 'docx', 'csv', 'txt'], accept_multiple_files=True)

    if uploaded_file:
        # ì—…ë¡œë“œëœ íŒŒì¼ì„ ì €ì¥
        doc_list = []
        for doc in uploaded_file:
            file_name = doc.name
            # íŒŒì¼ì„ write binary(wb)ëª¨ë“œë¡œ ì—´ê¸°
            with open(file_name, "wb") as file:
                file.write(doc.getvalue())
                logger.info(f"Uploaded {file_name}")
            try:
                if file_name.endswith('.pdf'):
                    loader = PyPDFLoader(file_name)
                elif file_name.endswith('.docx'):
                    loader = Docx2txtLoader(file_name)
                elif file_name.endswith('.csv'):
                    loader = CSVLoader(file_name)
                elif file_name.endswith('.txt'):
                    loader = TextLoader(file_name, encoding="utf-8")
                else:
                    st.error("Unsupported file type.")
                    continue
                
                # ë¬¸ì„œë¥¼ load and split -> chunk
                documents = loader.load()
                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
                splitted_documents = text_splitter.split_documents(documents)
                doc_list.extend(splitted_documents)

                st.write("File has been uploaded!")
            except Exception as e:
                st.error(f"Error loading {file_name}: {e}")
                logger.error(f"Error loading {file_name}: {e}")

        # RAG ê´€ë ¨ ë³€ìˆ˜ ì´ˆê¸°í™”
        if "vectorstore" not in st.session_state:
            if doc_list:
                try:
                    embeddings = OpenAIEmbeddings()
                    st.session_state["vectorstore"] = FAISS.from_documents(doc_list, embeddings)
                except Exception as e:
                    st.error(f"Error initializing vector store: {e}")
                    logger.error(f"Error initializing vector store: {e}")

        if "retriever" not in st.session_state:
            try:
                # ìœ ì‚¬ë„ ë†’ì€ K ê°œì˜ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
                k = 2
                # bm25 retriever(Sparse) and faiss retriever(Dense)ë¥¼ ì´ˆê¸°í™” í•©ë‹ˆë‹¤.
                bm25_retriever = BM25Retriever.from_documents(doc_list)
                bm25_retriever.k = k

                faiss_vectorstore = FAISS.from_documents(doc_list, OpenAIEmbeddings())
                faiss_retriever = faiss_vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 5, "score_threshold": 0.8})

                ensemble_retriever = EnsembleRetriever(
                    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]
                )
                st.session_state["retriever"] = ensemble_retriever
            except Exception as e:
                st.error(f"Error initializing retriever: {e}")
                logger.error(f"Error initializing retriever: {e}")

    if st.button("Reset"):
        # st.session_state["message"] = []
        st.rerun()

# ë©”ì„¸ì§€ ë‚´ìš©ì„ ê¸°ë¡í•˜ëŠ” ìƒíƒœ ë³€ìˆ˜
if "message" not in st.session_state:
    st.session_state["message"] = [] 

# ì±„íŒ… ê¸°ë¡ì„ ì €ì¥í•˜ëŠ” ìƒíƒœ ë³€ìˆ˜
if "store" not in st.session_state:
    st.session_state["store"] = dict()

# ì±„íŒ… ê¸°ë¡ì„ ì¶œë ¥í•˜ëŠ” ë¶€ë¶„ì„ í•¨ìˆ˜í™”
print_message()

# chat logic
if user_input := st.chat_input("ë©”ì„¸ì§€ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”"):
    # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‚´ìš©
    st.chat_message("user").write(f"{user_input}")
    st.session_state["message"].append(ChatMessage(role="user", content=user_input))

    with st.chat_message("assistant"):
        # RAG
        if "retriever" in st.session_state:
            try:
                # Retriever ì„¤ì •
                retriever = st.session_state["vectorstore"].as_retriever(search_type="mmr", search_kwargs={"k": 2, "score_threshold": 0.8})
                relevant_docs = retriever.get_relevant_documents(user_input)

                # LLM ì‘ë‹µ ìƒì„±
                # agent ì •ì˜
                search = TavilySearchResults(k=3)

                tool = create_retriever_tool(
                    retriever=retriever,  # í˜„ì¬ ì„¸ì…˜ì˜ retriever ì‚¬ìš©
                    name="search_documents",  # ë„êµ¬ ì´ë¦„
                    description="Searches and returns relevant excerpts from the uploaded documents."  # ë„êµ¬ ì„¤ëª…
                )
                tools = [search, tool]

                # streaming ì¶œë ¥ ìë¦¬
                stream_handler = StreamHandler(st.empty())
                llm = ChatOpenAI(model="gpt-4o-mini", streaming=True, callbacks=[stream_handler])
                
                agent_prompt = hub.pull("hwchase17/openai-functions-agent")
                agent = create_openai_functions_agent(llm, tools, agent_prompt)

                agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
                # sessionì— ì €ì¥
                if "agent_excutor" not in st.session_state:
                    st.session_state["agent_executor"] = agent_executor

                agent_executor = st.session_state["agent_executor"]
                # ì±„íŒ… ë©”ì‹œì§€ ê¸°ë¡ì´ ì¶”ê°€ëœ ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
                agent_with_chat_history = RunnableWithMessageHistory(
                    agent_executor,
                    get_session_history,    # chat historyë¥¼ ë¶ˆëŸ¬ì˜´
                    input_messages_key="input",
                    # í”„ë¡¬í”„íŠ¸ì˜ ë©”ì‹œì§€ê°€ ì…ë ¥ë˜ëŠ” key: "chat_history"
                    history_messages_key="chat_history"
                    )
    
                # AIì˜ ë‹µë³€ í‘œì‹œ
                # ì‘ë‹µ ìƒì„±
                response = agent_with_chat_history.invoke(
                    {"input": user_input},
                    config={"configurable": {"session_id": "MyTestSessionID"}}
                )
                answer = response["output"]
                # ì°¸ê³  ë¬¸ì„œ í‘œì‹œ
                with st.expander("ì°¸ê³  ë¬¸ì„œ"):
                    for doc in relevant_docs:
                        st.markdown(doc.metadata['source'], help=doc.page_content)

                st.session_state["message"].append(ChatMessage(role="assistant", content=answer))
            except Exception as e:
                st.error(f"Error during processing: {e}")
                logger.error(f"Error during processing: {e}")
        else:
            st.error("Retriever is not initialized.")






# ì¤‘ë³µ ì½”ë“œ ì œê±°
# streaming ë°©ì‹ìœ¼ë¡œ ì‘ë‹µì„ ìƒì„±í•˜ë„ë¡ Stream_handler ê¸°ëŠ¥ ì¶”ê°€
