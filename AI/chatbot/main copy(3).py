from loguru import logger
import streamlit as st
from utills import print_message, get_session_history, StreamHandler

from langchain import hub
from langchain_core.messages import ChatMessage
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, CSVLoader, TextLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.retrievers import BM25Retriever, EnsembleRetriever

st.set_page_config(page_title="MyAssistant", page_icon="ğŸ¤—")
st.title("ğŸ¤— MyAssistant")

if "conversation_chain" not in st.session_state:
    st.session_state.conversation = None

# ì‚¬ì´ë“œ ë°”
with st.sidebar:
    session_id = st.text_input("Session ID", value="Chating Room")
    uploaded_file = st.file_uploader("Upload a file", type=['pdf', 'docx', 'csv', 'txt'], accept_multiple_files=True)

    if uploaded_file is not None:
        doc_list = []
        for doc in uploaded_file:
            file_name = doc.name
            with open(file_name, "wb") as file:
                file.write(doc.getvalue())
                logger.info(f"Uploaded {file_name}")
            try:
                if file_name.endswith('.pdf'):
                    loader = PyPDFLoader(file_name)
                elif file_name.endswith('.docx'):
                    loader = Docx2txtLoader(file_name)
                elif file_name.endswith('.csv'):
                    loader = CSVLoader(file_name)
                elif file_name.endswith('.txt'):
                    loader = TextLoader(file_name, encoding="utf-8")
                else:
                    st.error("Unsupported file type.")
                    continue
                
                documents = loader.load()
                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
                splitted_documents = text_splitter.split_documents(documents)
                doc_list.extend(splitted_documents)
            
            except Exception as e:
                st.error(f"Error loading {file_name}: {e}")
                logger.error(f"Error loading {file_name}: {e}")

        # RAG ê´€ë ¨ ë³€ìˆ˜ ì´ˆê¸°í™”
        if "vectorstore" not in st.session_state:
            if doc_list:
                try:
                    embeddings = OpenAIEmbeddings()
                    st.session_state["vectorstore"] = FAISS.from_documents(doc_list, embeddings)
                except Exception as e:
                    st.error(f"Error initializing vector store: {e}")
                    logger.error(f"Error initializing vector store: {e}")

        if "retriever" not in st.session_state:
            try:
                # ìœ ì‚¬ë„ ë†’ì€ K ê°œì˜ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
                k = 2

                if not doc_list:
                    raise ValueError("doc_list is empty. Make sure documents are loaded before initializing the retriever.")

                # (Sparse) bm25 retriever and (Dense) faiss retriever ë¥¼ ì´ˆê¸°í™” í•©ë‹ˆë‹¤.
                bm25_retriever = BM25Retriever.from_documents(doc_list)
                bm25_retriever.k = k

                faiss_vectorstore = FAISS.from_documents(doc_list, OpenAIEmbeddings())
                faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={"k": k})

                # initialize the ensemble retriever
                ensemble_retriever = EnsembleRetriever(
                    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]
                )
                st.session_state["retriever"] = ensemble_retriever
            except Exception as e:
                st.error(f"Error initializing retriever: {e}")
                logger.error(f"Error initializing retriever: {e}")

    if st.button("Reset"):
        st.session_state["message"] = []
        st.rerun()

# ë©”ì„¸ì§€ ë‚´ìš©ì„ ê¸°ë¡í•˜ëŠ” ìƒíƒœ ë³€ìˆ˜
if "message" not in st.session_state:
    st.session_state["message"] = [] 

# ì±„íŒ… ê¸°ë¡ì„ ì €ì¥í•˜ëŠ” ìƒíƒœ ë³€ìˆ˜
if "store" not in st.session_state:
    st.session_state["store"] = dict()

# ì±„íŒ… ê¸°ë¡ì„ ì¶œë ¥í•˜ëŠ” ë¶€ë¶„ì„ í•¨ìˆ˜í™”
print_message()

# initialize chat box
if user_input := st.chat_input("ë©”ì„¸ì§€ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”"):
    # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‚´ìš©
    st.chat_message("user").write(f"{user_input}")
    st.session_state["message"].append(ChatMessage(role="user", content=user_input))

    # RAG ê¸°ëŠ¥: ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    if "retriever" in st.session_state:
        try:
            # ìœ ì‚¬ë„ ë†’ì€ K ê°œì˜ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
            k = 2

            relevant_docs = st.session_state["vectorstore"].similarity_search(user_input, k=k)
            if not relevant_docs:
                st.warning("ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                st.stop()

            # ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
            documents_text = "\n".join([doc.page_content for doc in relevant_docs])

            # LLM ì‘ë‹µ ìƒì„±
            llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
            prompt = hub.pull("rlm/rag-prompt")

            # contextë¥¼ Runnable í˜•ì‹ìœ¼ë¡œ ë˜í•‘
            context = {"context": documents_text, "question": user_input}

            # ì²´ì¸ ìƒì„±
            rag_chain = (
                prompt | llm
            )

            chain_with_memory = RunnableWithMessageHistory(
                rag_chain,
                get_session_history,
                input_messages_key="question",
                history_messages_key="history",
            )

            response = chain_with_memory.invoke(
                context,
                config={"configurable": {"session_id": session_id}}
            )
            answer = response.content

            # AIì˜ ë‹µë³€
            with st.chat_message("assistant"):
                stream_handler = StreamHandler(st.empty())
                llm = ChatOpenAI(model="gpt-4o-mini", streaming=True, callbacks=[stream_handler])
                prompt = ChatPromptTemplate.from_messages(
                    [
                        ("system", "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§§ê³  ê°„ê²°í•œ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”."),
                        MessagesPlaceholder(variable_name="history"),
                        ("human", "{question}")  # ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš© ì¶”ê°€
                    ]
                )
                chain = prompt | llm
                chain_with_callbacks = RunnableWithMessageHistory(
                    chain,
                    get_session_history,
                    input_messages_key="question",
                    history_messages_key="history",
                )
                response = chain_with_callbacks.invoke(
                    {"question": user_input},
                    config={"configurable": {"session_id": session_id}}
                )

                # ì°¸ê³  ë¬¸ì„œ í‘œì‹œ
                with st.expander("ì°¸ê³  ë¬¸ì„œ"):
                    for doc in relevant_docs:
                        st.markdown(doc.metadata['source'], help=doc.page_content)

                st.session_state["message"].append(ChatMessage(role="assistant", content=answer))
        except Exception as e:
            st.error(f"Error during processing: {e}")
            logger.error(f"Error during processing: {e}")
    else:
        st.error("Retriever is not initialized.")





# retriever ê¸°ëŠ¥ ì •ìƒí™”
# ensemble retrieverë¡œ ê²€ìƒ‰ì„±ëŠ¥ ì—…ê·¸ë ˆì´ë“œ
# ì‘ë‹µ ìƒì„±ë¶€ë¶„ ì½”ë“œ ê°€ë…ì„±
