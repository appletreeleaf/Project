from loguru import logger
import streamlit as st
from utills import print_message, get_session_history, get_conversation_chain, get_conversation_chain_with_callbacks, StreamHandler

from langchain_core.messages import ChatMessage
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, CSVLoader, UnstructuredPowerPointLoader, TextLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS

st.set_page_config(page_title="MyAssistant", page_icon="ğŸ¤—")
st.title("ğŸ¤— MyAssistant")

# side bar
with st.sidebar:
    session_id = st.text_input("Session ID", value="Chating Room")
    uploaded_file = st.file_uploader("Upload a file", type=['pdf', 'docx', 'csv', 'txt'], accept_multiple_files=True)

    if uploaded_file is not None:
        doc_list = []
        for doc in uploaded_file:
            file_name = doc.name
            with open(file_name, "wb") as file:
                file.write(doc.getvalue())
                logger.info(f"Uploaded {file_name}")
            try:
                if '.pdf' in doc.name:
                    loader = PyPDFLoader(file_name)
                    documents = loader.load_and_split()
                elif '.docx' in doc.name:
                    loader = Docx2txtLoader(file_name)
                    documents = loader.load_and_split()
                elif '.csv' in doc.name:
                    loader = CSVLoader(file_name)
                    documents = loader.load_and_split()
                elif '.txt' in doc.name:
                    loader = TextLoader(file_name, encoding="utf-8")
                    documents = loader.load_and_split()
                else:
                    st.error("Unsupported file type.")
                    continue

                doc_list.extend(documents)
            except Exception as e:
                st.error(f"Error loading {file_name}: {e}")
                logger.error(f"Error loading {file_name}: {e}")

        if st.button("Process") and doc_list:
            st.write("Document has been uploaded!")
        elif not doc_list:
            st.warning("No documents were loaded. Please check the file format.")

    if st.button("Reset"):
        st.session_state["message"] = []
        st.experimental_rerun()

# ë©”ì„¸ì§€ ë‚´ìš©ì„ ê¸°ë¡í•˜ëŠ” ìƒíƒœ ë³€ìˆ˜
if "message" not in st.session_state:
    st.session_state["message"] = [] 

# ì±„íŒ… ê¸°ë¡ì„ ì €ì¥í•˜ëŠ” ìƒíƒœ ë³€ìˆ˜
if "store" not in st.session_state:
    st.session_state["store"] = dict()

# RAG ê´€ë ¨ ë³€ìˆ˜ ì´ˆê¸°í™”
if "vector_store" not in st.session_state:
    if doc_list:
        try:
            embeddings = OpenAIEmbeddings()
            st.session_state["vector_store"] = FAISS.from_documents(doc_list, embeddings)
        except Exception as e:
            st.error(f"Error initializing vector store: {e}")
            logger.error(f"Error initializing vector store: {e}")

# ì±„íŒ… ê¸°ë¡ì„ ì¶œë ¥í•˜ëŠ” ë¶€ë¶„ì„ í•¨ìˆ˜í™”
print_message()

# initialize chat box
if user_input := st.chat_input("ë©”ì„¸ì§€ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”"):

    # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‚´ìš©
    st.chat_message("user").write(f"{user_input}")
    st.session_state["message"].append(ChatMessage(role="user", content=user_input))

    # RAG ê¸°ëŠ¥: ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    if "vector_store" in st.session_state:
        try:
            relevant_docs = st.session_state["vector_store"].similarity_search(user_input)

            # LLM ì‘ë‹µ ìƒì„±
            llm = ChatOpenAI(model="gpt-4o-mini")
            prompt = ChatPromptTemplate.from_messages(
                [
                    ("system", "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§§ê³  ê°„ê²°í•œ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”."),
                    MessagesPlaceholder(variable_name="history"),
                    ("human", "{question}"),  # ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš© ì¶”ê°€
                ]
            )

            chain = prompt | llm
            chain_with_memory = RunnableWithMessageHistory(
                chain,
                get_session_history,
                input_messages_key="question",
                history_messages_key="history",
            )

            response = chain_with_memory.invoke(
                {"question": user_input},
                config={"configurable": {"session_id": session_id}}
            )
            answer = response.content

            # AIì˜ ë‹µë³€
            with st.chat_message("assistant"):
                stream_handler = StreamHandler(st.empty())
                llm = ChatOpenAI(model="gpt-4o-mini", streaming=True, callbacks=[stream_handler])
                prompt = ChatPromptTemplate.from_messages(
                    [
                        ("system", "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§§ê³  ê°„ê²°í•œ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”."),
                        MessagesPlaceholder(variable_name="history"),
                        ("human", "{question}") # ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš© ì¶”ê°€
                    ]
                )
                chain = prompt | llm
                chain_with_callbacks = RunnableWithMessageHistory(
                    chain,
                    get_session_history,
                    input_messages_key="question",
                    history_messages_key="history",
                )
                response = chain_with_callbacks.invoke(
                    {"question": user_input},
                    config={"configurable": {"session_id": session_id}}
                )
                st.session_state["message"].append(ChatMessage(role="assistant", content=answer))
        except Exception as e:
            st.error(f"Error during processing: {e}")
            logger.error(f"Error during processing: {e}")
    else:
        st.error("Vector store is not initialized.")
