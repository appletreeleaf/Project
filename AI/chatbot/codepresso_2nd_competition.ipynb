{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 1 llm basic invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seoul'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# step 1\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# step 2\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You must answer the following question in one word.\n",
    "    #Question:\n",
    "    {question}\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# step 5\n",
    "# chain의 순서는 의미가 있구나\n",
    "# | 연산자는 순차적으로 연산 결과를 다음 인자에 전달하는 것이기 떄문이다..\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "question = \"Where is the capital of South Korea?\"\n",
    "chain.invoke(question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 2 few shot prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 질문에 추가 질문이 필요한가요: 예.  \n",
      "추가 질문: Google은 몇 년에 창립되었나요?  \n",
      "중간 답변: Google은 1998년에 창립되었습니다.  \n",
      "추가 질문: Bill Gates는 몇 년에 태어났나요?  \n",
      "중간 답변: Bill Gates는 1955년에 태어났습니다.  \n",
      "추가 질문: 1998년에는 Bill Gates가 몇 살이었나요?  \n",
      "중간 답변: 1998년에는 Bill Gates가 43세였습니다.  \n",
      "최종 답변은: 43세\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import FewShotPromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"스티브 잡스와 아인슈타인 중 누가 더 오래 살았나요?\",\n",
    "        \"answer\": \"\"\"이 질문에 추가 질문이 필요한가요: 예.\n",
    "추가 질문: 스티브 잡스는 몇 살에 사망했나요?\n",
    "중간 답변: 스티브 잡스는 56세에 사망했습니다.\n",
    "추가 질문: 아인슈타인은 몇 살에 사망했나요?\n",
    "중간 답변: 아인슈타인은 76세에 사망했습니다.\n",
    "최종 답변은: 아인슈타인\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"네이버의 창립자는 언제 태어났나요?\",\n",
    "        \"answer\": \"\"\"이 질문에 추가 질문이 필요한가요: 예.\n",
    "추가 질문: 네이버의 창립자는 누구인가요?\n",
    "중간 답변: 네이버는 이해진에 의해 창립되었습니다.\n",
    "추가 질문: 이해진은 언제 태어났나요?\n",
    "중간 답변: 이해진은 1967년 6월 22일에 태어났습니다.\n",
    "최종 답변은: 1967년 6월 22일\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"율곡 이이의 어머니가 태어난 해의 통치하던 왕은 누구인가요?\",\n",
    "        \"answer\": \"\"\"이 질문에 추가 질문이 필요한가요: 예.\n",
    "추가 질문: 율곡 이이의 어머니는 누구인가요?\n",
    "중간 답변: 율곡 이이의 어머니는 신사임당입니다.\n",
    "추가 질문: 신사임당은 언제 태어났나요?\n",
    "중간 답변: 신사임당은 1504년에 태어났습니다.\n",
    "추가 질문: 1504년에 조선을 통치한 왕은 누구인가요?\n",
    "중간 답변: 1504년에 조선을 통치한 왕은 연산군입니다.\n",
    "최종 답변은: 연산군\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"올드보이와 기생충의 감독이 같은 나라 출신인가요?\",\n",
    "        \"answer\": \"\"\"이 질문에 추가 질문이 필요한가요: 예.\n",
    "추가 질문: 올드보이의 감독은 누구인가요?\n",
    "중간 답변: 올드보이의 감독은 박찬욱입니다.\n",
    "추가 질문: 박찬욱은 어느 나라 출신인가요?\n",
    "중간 답변: 박찬욱은 대한민국 출신입니다.\n",
    "추가 질문: 기생충의 감독은 누구인가요?\n",
    "중간 답변: 기생충의 감독은 봉준호입니다.\n",
    "추가 질문: 봉준호는 어느 나라 출신인가요?\n",
    "중간 답변: 봉준호는 대한민국 출신입니다.\n",
    "최종 답변은: 예\n",
    "\"\"\",\n",
    "    },\n",
    "]\n",
    "example_prompt = PromptTemplate.from_template(\n",
    "    \"question:\\n{question}, Answer:\\n {answer}\"\n",
    ")\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"question:\\n{question}, Answer:\\n\",\n",
    "    input_variables=[\"question\"]\n",
    ")\n",
    "chain = prompt | llm | parser\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "question = \"Google이 창립된 연도에 Bill Gates의 나이는 몇 살인가요?\"\n",
    "\n",
    "print(f\"{chain.invoke({'question': question})}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. basic rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Word2vec is a group of related models used in natural language processing (NLP) to produce word embeddings, which are dense vector representations of words. Developed by a team led by Tomas Mikolov at Google in 2013, Word2vec uses neural networks to learn the relationships between words based on their context in large text corpora.\\n\\nThere are two main architectures for training Word2vec:\\n\\n1. **Continuous Bag of Words (CBOW)**: This model predicts a target word based on its surrounding context words. It takes the context words as input and tries to predict the center word.\\n\\n2. **Skip-gram**: This model works in the opposite way; it uses a target word to predict the surrounding context words. It is particularly effective for capturing relationships between words, especially for infrequent words.\\n\\nWord2vec is widely used because it can capture semantic meanings and relationships between words, allowing for tasks such as word similarity, analogy reasoning, and improving the performance of various NLP applications. The resulting word vectors can be used in various machine learning models and algorithms.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableMap\n",
    "\n",
    "loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "documents = loader.load_and_split(text_splitter=text_splitter)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma(embedding_function=embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"You are an assistant. Please answer the question following context. \\n#Context:\\n{context}\"),\n",
    "        (\"user\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# chain\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | parser\n",
    ")\n",
    "question = \"What is Word2vec?\"\n",
    "chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1923412328.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    import langchain_openai import ChatOpenAI\u001b[0m\n\u001b[1;37m                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def generate_questions_from_text(text):\n",
    "    \"\"\"\n",
    "    Generate questions from the given text using OpenAI's GPT model.\n",
    "\n",
    "    Args:\n",
    "        text: The input text from which to generate questions.\n",
    "\n",
    "    Returns:\n",
    "        List of generated questions.\n",
    "    \"\"\"\n",
    "    prompt = f\"Based on the following text, generate several questions:\\n\\n{text}\\n\\nQuestions:\"\n",
    "    \n",
    "    response = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    \n",
    "    questions = [choice['message']['content'] for choice in response['choices']]\n",
    "    return questions\n",
    "\n",
    "# 사용 예시\n",
    "doc_content = \"Your document content here.\"\n",
    "questions = generate_questions_from_text(doc_content)\n",
    "for question in questions:\n",
    "    print(question)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-cqASS06s-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
