{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "169cbdfaaa0943e4ae99cf8071826e73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_301cf29daf9442d9b8e2d2ba6fd3927f",
              "IPY_MODEL_9b44524f858d4b68bff2278b15833000",
              "IPY_MODEL_e0fdc52bed9349adbbf5d2ce7d30ff7d"
            ],
            "layout": "IPY_MODEL_04e7c50fe9dc4f9d8661a6ba4f48450f"
          }
        },
        "301cf29daf9442d9b8e2d2ba6fd3927f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4bed7362420407b832c4998da7d3b75",
            "placeholder": "​",
            "style": "IPY_MODEL_306e3f133d9544f18d5adc812c606b67",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "9b44524f858d4b68bff2278b15833000": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_681e11fdec2144d7889355bc7d3ce705",
            "max": 2825034,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_897ad19b051d4dce890a3f66f23e41f4",
            "value": 2825034
          }
        },
        "e0fdc52bed9349adbbf5d2ce7d30ff7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f979d6c9e114921b7bbd78b32ea1a42",
            "placeholder": "​",
            "style": "IPY_MODEL_877b7d30809442fb9cf864a4b688bdb0",
            "value": " 2.83M/2.83M [00:00&lt;00:00, 30.4MB/s]"
          }
        },
        "04e7c50fe9dc4f9d8661a6ba4f48450f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4bed7362420407b832c4998da7d3b75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "306e3f133d9544f18d5adc812c606b67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "681e11fdec2144d7889355bc7d3ce705": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "897ad19b051d4dce890a3f66f23e41f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f979d6c9e114921b7bbd78b32ea1a42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "877b7d30809442fb9cf864a4b688bdb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45a1cb8d4dab4519b5d16192d9791224": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3747a869832b412ca283fd86fffa73b1",
              "IPY_MODEL_366d140d4bdb4bbebca90bf2f7205034",
              "IPY_MODEL_a5662df2e3e3431d8f1937a847471ba3"
            ],
            "layout": "IPY_MODEL_a2ee35ec70374b8795a5797c4249a1e6"
          }
        },
        "3747a869832b412ca283fd86fffa73b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_461ee5bca30d42deae1b4357ec17d01e",
            "placeholder": "​",
            "style": "IPY_MODEL_bc61a32d6012444b83ac8e5e4c889ceb",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "366d140d4bdb4bbebca90bf2f7205034": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cceb9f28d91b4bbebd0f8f1b8390c3cd",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5213d81747c4e16b59355e830fe72ff",
            "value": 1000
          }
        },
        "a5662df2e3e3431d8f1937a847471ba3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d50ebc657c2485e99f3623acae1c3e0",
            "placeholder": "​",
            "style": "IPY_MODEL_bcd4900203d34cef89e1d8fee2078315",
            "value": " 1.00k/1.00k [00:00&lt;00:00, 51.0kB/s]"
          }
        },
        "a2ee35ec70374b8795a5797c4249a1e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "461ee5bca30d42deae1b4357ec17d01e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc61a32d6012444b83ac8e5e4c889ceb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cceb9f28d91b4bbebd0f8f1b8390c3cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5213d81747c4e16b59355e830fe72ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1d50ebc657c2485e99f3623acae1c3e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcd4900203d34cef89e1d8fee2078315": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/appletreeleaf/Projects/blob/main/Project3_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X52V1K9xKJBq"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import wandb\n",
        "import random\n",
        "from easydict import EasyDict as edict\n",
        "\n",
        "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import dataloader"
      ],
      "metadata": {
        "id": "26FNuew5OGrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wandb Setting"
      ],
      "metadata": {
        "id": "JOt2ttwyNNvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login() # API key = bcc72b6f4c340bf778bb3310374ab78fc660206e"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "fbbxRF_vNNZl",
        "outputId": "5dd288bb-3b2a-4556-ddb0-0aa66363cec7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args = edict({'w_project': 'test_project_3_JY',  #goorm_project_3\n",
        "              'w_entity': 'goorm_team_1',\n",
        "              'learning_rate': 5e-5,\n",
        "              'batch_size': {'train': 8,\n",
        "                             'eval': 8,\n",
        "                             'test': 8},\n",
        "              'accumulate': 4,     # 32개씩\n",
        "              'epochs': 1,\n",
        "              'seed': 42,\n",
        "              'model_name': 'skt/kogpt2-base-v2',\n",
        "              'max_length': 1024})\n",
        "\n",
        "args['NAME'] = ''f'skt/KoGPT2-base-v2{args.epochs}_max{args.max_length}_lr{args.learning_rate}_{random.randrange(0, 1024)}'\n",
        "print(args.NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y086iBHPNVMl",
        "outputId": "498340b8-e43c-46f8-8728-2485ec2a74e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skt/KoGPT2-base-v21_max1024_lr5e-05_278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BOS = '</s>'\n",
        "EOS = '</s>'\n",
        "UNK = '<unk>'\n"
      ],
      "metadata": {
        "id": "IJwddjS6BmYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class DialogKoGPT2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DialogKoGPT2, self).__init__()\n",
        "    self.kogpt2 = GPT2LMHeadModel.from_pretrained(\"skt/kogpt2-base-v2\")\n",
        "\n",
        "  def generate(self,\n",
        "               input_ids,\n",
        "               do_sample=True,\n",
        "               max_length= 50,\n",
        "               top_p=0.92,\n",
        "               top_k=50,\n",
        "               temperature= 0.6,\n",
        "               no_repeat_ngram_size =None,\n",
        "               num_return_sequences=3,\n",
        "               early_stopping=False,\n",
        "               ):\n",
        "    return self.kogpt2.generate(input_ids,\n",
        "               do_sample=do_sample,\n",
        "               max_length=max_length,\n",
        "               top_p = top_p,\n",
        "               top_k=top_k,\n",
        "               temperature=temperature,\n",
        "               no_repeat_ngram_size= no_repeat_ngram_size,\n",
        "               num_return_sequences=num_return_sequences,\n",
        "               early_stopping = early_stopping,\n",
        "              )\n",
        "\n",
        "  def forward(self, input, labels = None):\n",
        "    if labels is not None:\n",
        "      outputs = self.kogpt2(input, labels=labels)\n",
        "    else:\n",
        "      outputs = self.kogpt2(input)\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "XP20-NLhODCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
        "  bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
        "  pad_token='<pad>', mask_token='<mask>')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "169cbdfaaa0943e4ae99cf8071826e73",
            "301cf29daf9442d9b8e2d2ba6fd3927f",
            "9b44524f858d4b68bff2278b15833000",
            "e0fdc52bed9349adbbf5d2ce7d30ff7d",
            "04e7c50fe9dc4f9d8661a6ba4f48450f",
            "b4bed7362420407b832c4998da7d3b75",
            "306e3f133d9544f18d5adc812c606b67",
            "681e11fdec2144d7889355bc7d3ce705",
            "897ad19b051d4dce890a3f66f23e41f4",
            "7f979d6c9e114921b7bbd78b32ea1a42",
            "877b7d30809442fb9cf864a4b688bdb0",
            "45a1cb8d4dab4519b5d16192d9791224",
            "3747a869832b412ca283fd86fffa73b1",
            "366d140d4bdb4bbebca90bf2f7205034",
            "a5662df2e3e3431d8f1937a847471ba3",
            "a2ee35ec70374b8795a5797c4249a1e6",
            "461ee5bca30d42deae1b4357ec17d01e",
            "bc61a32d6012444b83ac8e5e4c889ceb",
            "cceb9f28d91b4bbebd0f8f1b8390c3cd",
            "c5213d81747c4e16b59355e830fe72ff",
            "1d50ebc657c2485e99f3623acae1c3e0",
            "bcd4900203d34cef89e1d8fee2078315"
          ]
        },
        "id": "krraMhvWPko5",
        "outputId": "cf04d3bc-1b88-4429-912c-3363e6478cdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.83M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "169cbdfaaa0943e4ae99cf8071826e73"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45a1cb8d4dab4519b5d16192d9791224"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WellnessAutoRegressiveDataset():      # raw data를 모델 입력형식에 맞게끔 변환\n",
        "  \"\"\"Wellness Auto Regressive Dataset\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               file_path1 = \"/content/drive/MyDrive/Groom_8th_project3/Data/wellness_dialog_for_autoregressive.txt\",\n",
        "               file_path2 = '/content/drive/MyDrive/Groom_8th_project3/Data/ChatbotData.txt',\n",
        "               file_path3 = '/content/drive/MyDrive/Groom_8th_project3/Data/data.txt',\n",
        "               n_ctx = 512, token = tokenizer):\n",
        "    self.file_path1 = file_path1\n",
        "    self.file_path2 = file_path2\n",
        "    self.file_path3 = file_path3\n",
        "    self.data =[]\n",
        "    self.q = []\n",
        "    self.a = []\n",
        "    self.tokenizer = token\n",
        "\n",
        "\n",
        "    bos_token_id = [self.tokenizer.bos_token_id] # word to inteager(token)\n",
        "    eos_token_id = [self.tokenizer.eos_token_id]\n",
        "    pad_token_id = [self.tokenizer.pad_token_id]\n",
        "\n",
        "    file1 = open(self.file_path1, 'r', encoding = \"utf-8\")\n",
        "    cnt = 0\n",
        "    while True:\n",
        "      line = file1.readline() # 한줄씩 읽음\n",
        "      if not line:            # 다 읽었으면 멈춤\n",
        "        break\n",
        "      if cnt > 0:\n",
        "        datas = line.split(\".,\") # .,기준으로 Q / A split\n",
        "        index_of_words = self.tokenizer.encode(datas[0]) + bos_token_id + self.tokenizer.encode(datas[1][:-1])+ eos_token_id\n",
        "        self.q.append(self.tokenizer.encode(datas[0]))\n",
        "        self.a.append(self.tokenizer.encode(datas[1][:-1]) + eos_token_id)\n",
        "\n",
        "\n",
        "        pad_token_len = n_ctx - len(index_of_words)       # padding\n",
        "        index_of_words += pad_token_id * pad_token_len    # max_len보다 작은 sequence에 padding\n",
        "        self.data.append(index_of_words)                  # 가공이 끝난 sequence를 data\n",
        "      cnt += 1\n",
        "    file1.close()\n",
        "\n",
        "    cnt1 = 0\n",
        "    file2 = open(self.file_path2, 'r', encoding='utf-8')\n",
        "    while True:\n",
        "      line = file2.readline()\n",
        "      if not line:\n",
        "        break\n",
        "      if cnt > 1:\n",
        "        datas = line.split(\",\")\n",
        "        index_of_words = self.tokenizer.encode(datas[0]) + bos_token_id + self.tokenizer.encode(datas[1][:-1])+ eos_token_id # q + <bos> + a + <eos>\n",
        "        self.q.append(self.tokenizer.encode(datas[0]))\n",
        "        self.a.append(self.tokenizer.encode(datas[1][:-1]) + eos_token_id)\n",
        "        pad_token_len = n_ctx - len(index_of_words)\n",
        "        index_of_words += pad_token_id * pad_token_len\n",
        "        self.data.append(index_of_words)\n",
        "      cnt += 1\n",
        "    file2.close()\n",
        "\n",
        "    file3 = open(self.file_path3, 'r', encoding='utf-8')\n",
        "    while True:\n",
        "      line = file3.readline()\n",
        "      if not line:\n",
        "        break\n",
        "      if cnt > 2:\n",
        "        datas = line.split(\",\")\n",
        "        index_of_words = self.tokenizer.encode(datas[0]) + bos_token_id + self.tokenizer.encode(datas[1][:-1])+ eos_token_id # q + <bos> + a + <eos>\n",
        "        self.q.append(self.tokenizer.encode(datas[0]))\n",
        "        self.a.append(self.tokenizer.encode(datas[1][:-1]) + eos_token_id)\n",
        "        pad_token_len = n_ctx - len(index_of_words)\n",
        "        index_of_words += pad_token_id * pad_token_len\n",
        "        self.data.append(index_of_words)\n",
        "      cnt += 1\n",
        "    file3.close()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # item = {}\n",
        "    item = self.data[index]\n",
        "    # label = self.a[index]\n",
        "    # item[\"label\"] = label\n",
        "\n",
        "    return item"
      ],
      "metadata": {
        "id": "rQs1Gh6iPZzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epoch = 1        # Num of Epoch\n",
        "batch_size = 8      # 배치 사이즈\n",
        "save_step = 100 # 학습 저장 주기\n",
        "learning_rate = 5e-5  # Learning Rate\n",
        "\n",
        "checkpoint_path =\"/content/drive/MyDrive/Groom_8th_project3/model\"\n",
        "save_ckpt_path = f\"{checkpoint_path}/kogpt2-wellnesee-auto-regressive_1.pth\""
      ],
      "metadata": {
        "id": "xlS9ZcAmPTjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project = args.w_project, entity = args.w_entity) # (project name, project_department)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "NiBXDGb3OoYt",
        "outputId": "431d36f9-58f4-4714-82b6-956fd519992f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m02younge\u001b[0m (\u001b[33mgoorm_team_1\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230411_063747-rw67sc8i</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/goorm_team_1/test_project_3_JY/runs/rw67sc8i' target=\"_blank\">brisk-salad-10</a></strong> to <a href='https://wandb.ai/goorm_team_1/test_project_3_JY' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/goorm_team_1/test_project_3_JY' target=\"_blank\">https://wandb.ai/goorm_team_1/test_project_3_JY</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/goorm_team_1/test_project_3_JY/runs/rw67sc8i' target=\"_blank\">https://wandb.ai/goorm_team_1/test_project_3_JY/runs/rw67sc8i</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/goorm_team_1/test_project_3_JY/runs/rw67sc8i?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f83d4860c70>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.run.name = args.NAME\n",
        "wandb.config.learning_rate = args.learning_rate\n",
        "wandb.config.epochs = args.epochs"
      ],
      "metadata": {
        "id": "yB40MO-NOqvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset= WellnessAutoRegressiveDataset()\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "jBxfdSl9PVan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.__getitem__(1))\n",
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAe01u8QPsqu",
        "outputId": "234a6e3d-71b9-4239-8c0b-cb88b2e69607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9037, 29524, 9510, 12107, 9031, 27511, 25856, 15812, 7489, 10704, 12258, 32738, 8006, 32286, 16959, 12537, 8146, 9183, 7251, 25856, 1, 9265, 7235, 9022, 9032, 7671, 10014, 8711, 25856, 9564, 9723, 11392, 8234, 406, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "130956"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = DialogKoGPT2()\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "T4iOSxKHQR8J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0797282-2e13-4dec-818b-a834337ff6a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DialogKoGPT2(\n",
              "  (kogpt2): GPT2LMHeadModel(\n",
              "    (transformer): GPT2Model(\n",
              "      (wte): Embedding(51200, 768)\n",
              "      (wpe): Embedding(1024, 768)\n",
              "      (drop): Dropout(p=0.1, inplace=False)\n",
              "      (h): ModuleList(\n",
              "        (0-11): 12 x GPT2Block(\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (attn): GPT2Attention(\n",
              "            (c_attn): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): GPT2MLP(\n",
              "            (c_fc): Conv1D()\n",
              "            (c_proj): Conv1D()\n",
              "            (act): NewGELUActivation()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_cross = torch.nn.CrossEntropyLoss(ignore_index=3)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "8G3uzFUAQVVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "for epoch in range(args.epochs):\n",
        "    count = 0\n",
        "    with tqdm(total=len(train_loader), desc=f\"Train({epoch})\") as pbar:\n",
        "        for i, data in enumerate(train_loader): # data = 길이 1024의 embedding vector\n",
        "            optimizer.zero_grad()     # initialize gradient\n",
        "            data = torch.stack(data)  # Tensor들로 구성된 list이기 때문에 list를 stack을 쌓아주고 (S_L, B)\n",
        "            data = data.transpose(1, 0) # (1, B, S_L)\n",
        "            data= data.to(device)\n",
        "\n",
        "            outputs = model(data, labels=data)\n",
        "            _, logits = outputs[:2]\n",
        "            # Shift so that tokens < n predict n\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_labels = data[..., 1:].contiguous()\n",
        "\n",
        "            loss = loss_cross(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            wandb.log({'train_loss': np.mean(losses)})\n",
        "\n",
        "\n",
        "            if count % save_step ==0 or (len(data) < batch_size):\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'train_no': count,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'loss': loss\n",
        "                }, save_ckpt_path)\n",
        "                model.kogpt2.save_pretrained(f'/content/drive/MyDrive/Groom_8th_project3{epoch}')\n",
        "            count += 1\n",
        "            pbar.update(1)\n",
        "            pbar.set_postfix_str(f\"Loss: {loss.item():.3f} ({np.mean(losses):.3f})\")"
      ],
      "metadata": {
        "id": "6n8FJ15hSVrS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40d07f1f-28d1-4f5b-bba3-61373d716772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train(0): 100%|██████████| 16370/16370 [1:34:03<00:00,  2.90it/s, Loss: 2.776 (2.658)]\n",
            "Train(1): 100%|██████████| 16370/16370 [1:36:47<00:00,  2.82it/s, Loss: 1.928 (2.444)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##테스트"
      ],
      "metadata": {
        "id": "lh8PJDyMBByj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n",
        "!pip install gtts"
      ],
      "metadata": {
        "id": "J0VMVBqNBBPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from typing import Dict, List\n",
        "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
        "import gtts\n",
        "\n",
        "\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
        "  bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
        "  pad_token='<pad>', mask_token='<mask>')\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"jack-oh/KoGPT2_finetuned_wellness\")\n",
        "model.eval()\n",
        "st.set_page_config(page_title='Project3', page_icon='translator-icon.png', layout='wide', initial_sidebar_state='expanded')\n",
        "st.title(\"Lyric Translator:balloon:\")\n",
        "\n",
        "\n",
        "\n",
        "# text = st.text_area(\"Enter text:\",height=None,max_chars=None,key=None,help=\"Enter your text here\")\n",
        "text = st.text_area(\"Enter text:\", placeholder=\"Enter your text here\")\n",
        "\n",
        "option1 = st.selectbox('Input language',\n",
        "                        ('english' , 'korean'))\n",
        "option2 = st.selectbox('Output language',\n",
        "                        ('korean' , 'english'))\n",
        "Languages = {'english':'en', 'korean':'ko'}\n",
        "value1 = Languages[option1] # 'en' or 'ko'\n",
        "value2 = Languages[option2] # 'en' or 'ko'\n",
        "\n",
        "if st.button('Translate Sentence'):\n",
        "    if text == \"\":\n",
        "        st.warning('Please **enter text** for translation')\n",
        "\n",
        "    else:\n",
        "        embeddings = tokenizer(text, return_attention_mask=False, return_token_type_ids=False, return_tensors='pt') #GPT의 input\n",
        "        output = en2kor_model.generate(**embeddings, max_length=50)[0, 1:-1]\n",
        "        output=en2kor_trg_tokenizer.decode(output, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "        st.text_area(\"Translation\", value=output, disabled=True)\n",
        "\n",
        "        converted_audio = gtts.gTTS(output, lang=value2)\n",
        "        converted_audio.save(\"translated.mp3\")\n",
        "        audio_file = open('translated.mp3','rb')\n",
        "        audio_bytes = audio_file.read()\n",
        "        st.audio(audio_bytes, format='audio')\n",
        "        st.write(\"To **download the audio file**, click the kebab menu on the audio bar.\")\n",
        "        st.success(\"Translation is **successfully** completed!\")\n",
        "        st.balloons()\n",
        "else:\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvfmdIRI4_2T",
        "outputId": "9d924df3-8069-4f40-dd29-8c7e98b7528c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
        "  bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
        "  pad_token='<pad>', mask_token='<mask>')\n",
        "model = GPT2LMHeadModel.from_pretrained(\"/content/drive/MyDrive/Groom_8th_project3/model/kogpt2-wellnesee-auto-regressive_1.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "id": "ORd8SSR5GIkb",
        "outputId": "fc6b7dd0-0ff6-4532-af43-034b75c450f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0;31m# Load config dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m             \u001b[0mconfig_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dict_from_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_config_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m             \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommit_hash\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_dict_from_json_file\u001b[0;34m(cls, json_file)\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 128: invalid start byte",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-5f273141367a>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mbos_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'</s>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meos_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'</s>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'<unk>'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   pad_token='<pad>', mask_token='<mask>')\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Groom_8th_project3/model/kogpt2-wellnesee-auto-regressive_1.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2174\u001b[0m             \u001b[0mconfig_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2175\u001b[0;31m             config, model_kwargs = cls.config_class.from_pretrained(\n\u001b[0m\u001b[1;32m   2176\u001b[0m                 \u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2177\u001b[0m                 \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"foo\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         ```\"\"\"\n\u001b[0;32m--> 546\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             logger.warning(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0moriginal_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    659\u001b[0m             \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommit_hash\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m             raise EnvironmentError(\n\u001b[0m\u001b[1;32m    662\u001b[0m                 \u001b[0;34mf\"It looks like the config file at '{resolved_config_file}' is not a valid JSON file.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             )\n",
            "\u001b[0;31mOSError\u001b[0m: It looks like the config file at '/content/drive/MyDrive/Groom_8th_project3/model/kogpt2-wellnesee-auto-regressive_1.pth' is not a valid JSON file."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "U_TKN = '<usr>'\n",
        "S_TKN = '<sys>'\n",
        "EOS = '</s>'\n",
        "MASK = '<unused0>'\n",
        "PAD = '<pad>'\n",
        "SENT = '<unused1>'\n",
        "\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
        "            eos_token=EOS, unk_token='<unk>',\n",
        "            pad_token=PAD, mask_token=MASK)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDcwWbvzIdcN",
        "outputId": "20737d0b-1f82-475f-bcff-f988ffa1ab39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chat():\n",
        "    with torch.no_grad():\n",
        "      print(\"채팅을 시작합니다. \\n\")\n",
        "      qs=[]\n",
        "      while 1:\n",
        "          q = input('나 > ').strip()\n",
        "          qs.append(q) # history 저장\n",
        "\n",
        "          if q == 'quit':\n",
        "              break\n",
        "          a=''\n",
        "          user = U_TKN + q + SENT + a         # question만 넣음\n",
        "          encoded = tokenizer.encode(user)\n",
        "          input_ids = torch.LongTensor(encoded).unsqueeze(dim=0)\n",
        "          output = model.generate(input_ids,\n",
        "                                  max_length=50,\n",
        "                                  num_beams=10,\n",
        "                                  do_sample=False,\n",
        "                                  top_k=50,\n",
        "                                  no_repeat_ngram_size=2,\n",
        "                                  temperature=0.85)\n",
        "          a=tokenizer.decode(output[0])\n",
        "          idx = torch.where(output[0]==tokenizer.encode('<sys>')[0])\n",
        "          chatbot = tokenizer.decode(output[0][int(idx[0])+1:], skip_special_tokens=True)\n",
        "\n",
        "          if '답변' in a: # 응, 아니 등이 input으로 들어왔을 때\n",
        "              a_new = ''\n",
        "              user = U_TKN + ''.join(qs[-2:]) + SENT + a_new # 직전 history 가지고 와서 sentiment 고려해주기\n",
        "              encoded = tokenizer.encode(user)\n",
        "              input_ids = torch.LongTensor(encoded).unsqueeze(dim=0)\n",
        "              output = model.generate(input_ids,max_length=50,\n",
        "                                        num_beams=10, do_sample=False,\n",
        "                                        top_k=50, no_repeat_ngram_size=2,\n",
        "                                      temperature=0.85)\n",
        "              a_new = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "              idx = torch.where(output[0]==tokenizer.encode('<sys>')[0])\n",
        "              chatbot = tokenizer.decode(output[0][int(idx[0])+1:], skip_special_tokens=True)\n",
        "\n",
        "              print(\"챗봇 > {}\".format(chatbot.strip()))\n",
        "\n",
        "          else:\n",
        "              print(\"챗봇 > {}\".format(chatbot.strip()))"
      ],
      "metadata": {
        "id": "Chg5ltCfGJFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulUB0s1uGdnU",
        "outputId": "87074a39-4fe7-4dd1-ca79-dd5131228a4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "채팅을 시작합니다. \n",
            "\n",
            "나 > 친구랑 싸웠어...\n",
            "챗봇 > 웃음이 줄어들고 눈물도 더 이상 나지 않아 감각이 없이 멍한 듯한 느낌이 드나요?\n",
            "나 > 응 조금 그런 거 같아\n",
            "챗봇 > 싸우면서 정 들 거예요.\n",
            "나 > 무슨 소리야\n",
            "챗봇 > 직설적으로 통보하는 말보다는 조금 더 부드럽게 권유하고 물어보는 말을 해보세요.\n",
            "나 > 내가 그랬어 근데 걔는 그러지 않았어\n",
            "챗봇 > 지속적으로 당신이 못났다고 생각하시나요?\n",
            "나 > 아니 난 잘났다고 생각해\n",
            "챗봇 > 지속적으로 당신을 비판하고 자책하시 성공하였다.\n",
            "당하였다.\n",
            " 감각이 없이 멍한 듯한 느껴지시나요?\n",
            "나 > quit\n"
          ]
        }
      ]
    }
  ]
}
